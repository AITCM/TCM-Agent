{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ratelimit gseapy rdkit pubchempy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import TCMAgent\n",
    "from tools.llm_api import *\n",
    "from tools.json_tool import *\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from tqdm import tqdm\n",
    "from tools.pubmed_api import PubMedFetcher\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_all_pages(\n",
    "    fetcher,\n",
    "    query: str,\n",
    "    max_pages: int = 5,\n",
    "    results_per_page: int = 10,\n",
    "    sleep_time: int = 3,\n",
    "    output_file: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    获取多页PubMed搜索结果并整合到一个字典中\n",
    "    \n",
    "    Args:\n",
    "        fetcher: PubMedFetcher实例\n",
    "        query: 搜索查询字符串\n",
    "        max_pages: 最大获取页数\n",
    "        results_per_page: 每页结果数0\n",
    "        sleep_time: 页面间暂停时间(秒)\n",
    "        output_file: 可选的输出JSON文件路径\n",
    "    \n",
    "    Returns:\n",
    "        包含所有文章和元数据的字典\n",
    "    \"\"\"\n",
    "    all_results = {\n",
    "        \"papers\": [],\n",
    "        \"metadata\": {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 获取第一页以获取总结果数\n",
    "        first_page = fetcher.search(\n",
    "            query=query,\n",
    "            max_results=results_per_page,\n",
    "            start=0\n",
    "        )\n",
    "        \n",
    "        total_results = first_page[\"metadata\"][\"total_results\"]\n",
    "        total_pages = min(max_pages, (total_results + results_per_page - 1) // results_per_page)\n",
    "        \n",
    "        print(f\"找到 {total_results} 篇文章，将获取 {total_pages} 页\")\n",
    "        \n",
    "        # 添加第一页结果\n",
    "        all_results[\"papers\"].extend(first_page[\"papers\"])\n",
    "        all_results[\"metadata\"] = {\n",
    "            \"total_results\": total_results,\n",
    "            \"pages_retrieved\": total_pages,\n",
    "            \"results_per_page\": results_per_page,\n",
    "            \"query\": query,\n",
    "            \"total_papers_retrieved\": len(first_page[\"papers\"])\n",
    "        }\n",
    "        \n",
    "        # 获取剩余页面\n",
    "        if total_pages > 1:\n",
    "            with tqdm(range(1, total_pages), desc=\"获取页面\") as pbar:\n",
    "                for page in pbar:\n",
    "                    time.sleep(sleep_time)  # 在请求之间暂停\n",
    "                    \n",
    "                    start_index = page * results_per_page\n",
    "                    try:\n",
    "                        result = fetcher.search(\n",
    "                            query=query,\n",
    "                            max_results=results_per_page,\n",
    "                            start=start_index\n",
    "                        )\n",
    "                        \n",
    "                        all_results[\"papers\"].extend(result[\"papers\"])\n",
    "                        all_results[\"metadata\"][\"total_papers_retrieved\"] = len(all_results[\"papers\"])\n",
    "                        \n",
    "                        pbar.set_postfix({\n",
    "                            \"已获取文章\": len(all_results[\"papers\"]),\n",
    "                            \"当前页文章数\": len(result[\"papers\"])\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"\\n获取第 {page + 1} 页时出错: {str(e)}\")\n",
    "                        continue\n",
    "        \n",
    "        # 如果指定了输出文件，保存结果\n",
    "        if output_file:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\n结果已保存至: {output_file}\")\n",
    "        \n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"获取过程中出错: {str(e)}\")\n",
    "        return all_results\n",
    "\n",
    "def format_results(paper):    \n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['title'] = [paper['title']]\n",
    "    temp_df['pmid'] = [paper['pmid']]\n",
    "    temp_df['journal'] = [paper['journal']['title']]\n",
    "    authors = \"\"\n",
    "    for author in paper['authors']:\n",
    "        authors += f\"{author['fore_name']} {author['last_name']}, \"\n",
    "        if author['affiliations']:\n",
    "            temp_df['affiliations'] = [author['affiliations'][0]]\n",
    "        else:\n",
    "            temp_df['affiliations'] = [None]\n",
    "    temp_df['authors'] = [authors]\n",
    "    if paper['abstract']['structured']:\n",
    "        for section, text in paper['abstract']['sections'].items():\n",
    "            temp_df[section] = [text]\n",
    "    else:\n",
    "        temp_df['abstract'] = [paper['abstract']['complete']]\n",
    "    \n",
    "    if paper['keywords']:\n",
    "        temp_df['keywords'] = [\", \".join(paper['keywords'])]\n",
    "    else:\n",
    "        temp_df['keywords'] = [None]\n",
    "    \n",
    "    for url_type, url in paper['urls'].items():\n",
    "        if url:\n",
    "            temp_df[url_type] = [url]\n",
    "    if paper['metadata']:\n",
    "        temp_df['metadata'] = [paper['metadata']]\n",
    "    else:\n",
    "        temp_df['metadata'] = [None]\n",
    "    \n",
    "    temp_df['is_open_access'] = [paper['metadata']['is_open_access']]\n",
    "\n",
    "    temp_df['fetch_time'] = [paper['metadata']['fetch_time']]\n",
    "    return temp_df\n",
    "\n",
    "# 组装LLM聊天记录信息，用于提取关键词\n",
    "def combine_context():\n",
    "    llm_ans = agent.conversations[-1][\"content\"]\n",
    "    human_question = agent.conversations[-4][\"content\"]\n",
    "    content = f\"\"\"\n",
    "    human_question：\n",
    "    {human_question}\n",
    "    ---\n",
    "    llm answer:\n",
    "    {llm_ans}\n",
    "    \"\"\"\n",
    "    return content\n",
    "\n",
    "def get_keywords(content, model=\"deepseek-chat\"):\n",
    "    PROMPT = \"\"\"\n",
    "# 你的任务\n",
    "基于我下面提供的聊天记录，提取与主题有关的的关键词\n",
    "\n",
    "# 关键词要求\n",
    "- 关键词必须是英文\n",
    "- 关键词控制在4个以内\n",
    "- 涉及到药物的必须要有相关药物的关键词，比如：中药复方的名称、疾病、关键成分、关键靶点\n",
    "- 你的输出必须是JSON，key必须是\"keywords\"\n",
    "\n",
    "# 示例\n",
    "用户问题：冠心宁注射液（丹参、川芎）治疗心力衰竭（Heart Failure, HF）的系统药理学分析，包括关键成分、关键靶点和KEGG、WikiPathways通路，系统解读分析药理机制结果\n",
    "你的输出：\n",
    "```json\n",
    "{{\n",
    "\"keywords\": [\"Guanxinning Injection\", \"Heart Failure\", \"Tanshinone Compounds (e.g., Tanshinone IIA)\"]\n",
    "}}\n",
    "```\n",
    "\n",
    "# 内容\n",
    "{content}\n",
    "\n",
    "现在，请输出关键词JSON：\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    prompt = PROMPT.format(content=content)\n",
    "    ans = \"\"\n",
    "    for char in get_llm_answer(prompt, model, temperature=0.95):\n",
    "        print(char, end=\"\", flush=True)\n",
    "        ans += char\n",
    "    print()\n",
    "    keywords = get_json(ans)[\"keywords\"]\n",
    "    return keywords\n",
    "\n",
    "# 获取搜索结果文本\n",
    "def get_search_results(keywords):\n",
    "    search_results_all = []\n",
    "    for i, query in enumerate(keywords):\n",
    "        print(f\"正在获取第{i+1}个关键词的检索结果：{query}\")\n",
    "        # 使用示例\n",
    "        fetcher = PubMedFetcher(api_key=os.getenv(\"PUBMED_API_KEY\"))\n",
    "\n",
    "        results = fetch_all_pages(\n",
    "            fetcher,\n",
    "            query=query,\n",
    "            max_pages=1,\n",
    "            results_per_page=5,\n",
    "            sleep_time=3,\n",
    "            output_file=\"pubmed_results.json\"\n",
    "        )\n",
    "\n",
    "        search_results_all.extend(results[\"papers\"])\n",
    "    # search_results_all获取每篇论文的标题及链接\n",
    "    searched_results = \"\"\n",
    "    for paper in search_results_all:\n",
    "        title = paper[\"title\"]\n",
    "        pubmed_url = paper[\"urls\"][\"pubmed\"]\n",
    "        searched_results += f\"title：{title}\\npubmed_url：{pubmed_url}\\n===\\n\"\n",
    "    return searched_results \n",
    "\n",
    "\n",
    "question = \"伊马替尼(Imatinib)与BCR-ABL激酶的结合活性是多少？\"\n",
    "question = \"冠心宁注射液（丹参、川芎）治疗心力衰竭（Heart Failure, HF）的系统药理学分析，包括关键成分、关键靶点和KEGG、WikiPathways通路，系统解读分析药理机制结果\"\n",
    "\n",
    "main_model = \"deepseek-chat\"\n",
    "tool_model = \"deepseek-chat\"\n",
    "flash_model = \"deepseek-chat\"\n",
    "\n",
    "agent = TCMAgent(main_model, tool_model, flash_model)\n",
    "\n",
    "intention_tools = agent.get_conversation_intention_tools(question)\n",
    "print(\"intention_tools:\", intention_tools)\n",
    "\n",
    "for char in agent.work_flow(question, intention_tools):\n",
    "    print(char, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content = combine_context()\n",
    "keywords = get_keywords(content)\n",
    "\n",
    "searched_results = get_search_results(keywords)\n",
    "print(searched_results)\n",
    "\n",
    "# 基于问题、AI回答、搜索结果文本，筛选出与问题相关的论文，展示论文标题及链接，markdown格式\n",
    "def get_related_papers(content, searched_results):\n",
    "    SELECT_PAPER_PROMPT = \"\"\"\n",
    "    # 你的任务\n",
    "    基于我下面提供的聊天记录，筛选出与问题相关的论文，以markdown格式展示论文标题及链接，\n",
    "\n",
    "    # 聊天记录\n",
    "    {content}  \n",
    "\n",
    "    # 搜索结果文本\n",
    "    {searched_results}\n",
    "\n",
    "    现在，请判断搜索结果中，哪些论文与问题相关，以markdown格式展示论文标题及链接，示例格式：[论文标题](论文链接)\n",
    "    你的输出格式：\n",
    "    # 相关论文\n",
    "    1. [论文标题](论文链接)\n",
    "    2. [论文标题](论文链接)\n",
    "    3. [论文标题](论文链接)\n",
    "    ...\n",
    "    \"\"\"\n",
    "    prompt = SELECT_PAPER_PROMPT.format(content=content, searched_results=searched_results)\n",
    "    ans = \"\"\n",
    "    for char in get_llm_answer(prompt, model=\"deepseek-chat\", temperature=0.95):\n",
    "        print(char, end=\"\", flush=True)\n",
    "        ans += char\n",
    "    print()\n",
    "    return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于问题、AI回答、搜索结果文本，筛选出与问题相关的论文，展示论文标题及链接，markdown格式\n",
    "def get_related_papers_yield(content, searched_results):\n",
    "    SELECT_PAPER_PROMPT = \"\"\"\n",
    "# 你的任务\n",
    "基于我下面提供的聊天记录，筛选出与问题相关的论文，以markdown格式展示论文标题及链接，\n",
    "\n",
    "# 聊天记录\n",
    "{content}  \n",
    "\n",
    "# 搜索结果文本\n",
    "{searched_results}\n",
    "\n",
    "现在，请判断搜索结果中，哪些论文与问题相关，以markdown格式展示论文标题及链接，示例格式：[论文标题](论文链接)\n",
    "你的输出格式：\n",
    "# 相关论文\n",
    "1. [论文标题](论文链接)\n",
    "2. [论文标题](论文链接)\n",
    "3. [论文标题](论文链接)\n",
    "...\n",
    "\"\"\"\n",
    "    prompt = SELECT_PAPER_PROMPT.format(content=content, searched_results=searched_results)\n",
    "    ans = \"\"\n",
    "    for char in get_llm_answer(prompt, model=\"deepseek-chat\", temperature=0.95):\n",
    "        ans += char\n",
    "        yield char \n",
    "    yield \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取data/conversation.json\n",
    "with open(\"data/conversation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    conversation = json.load(f)\n",
    "content = conversation[-1][\"content\"]\n",
    "# 保存content到data/content.txt\n",
    "with open(\"data/content.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.pathway_enrich import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "gpmap = GSEAPyDatabaseMap()\n",
    "user_input = \"请评估复元活血汤（柴胡半两、瓜蒌根、当归各、红花、甘草、大黄、桃仁）在纤维化中的作用，以及在kegg和GO、Reactome的通路富集情况\"\n",
    "databases = gpmap.get_databases(user_input)\n",
    "print(\"提取的数据库名称:\", databases)\n",
    "\n",
    "# 示例基因列表\n",
    "genes = [\n",
    "    \"TP53\", \"BRCA1\", \"EGFR\", \"MYC\", \"AKT1\", \n",
    "    \"VEGFA\", \"PTEN\", \"KRAS\", \"CDKN2A\", \"IL6\", \n",
    "    \"TNF\", \"MAPK1\", \"STAT3\", \"JUN\", \"FOS\", \n",
    "    \"HIF1A\", \"NFKB1\", \"PIK3CA\", \"RB1\", \"CCND1\"\n",
    "]\n",
    "\n",
    "# 定义要查找的疾病名称列表\n",
    "disease_names = ['Sclerosis', 'Cancer', 'Heart']\n",
    "\n",
    "agent2 = GeneEnrichmentAnalyzer()\n",
    "# 执行分析\n",
    "summary, enrichment_results = agent2.get_enrichment_summary(genes, gene_sets=databases, disease_names=disease_names)\n",
    "enrichment_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "def plot_terms_by_pvalue(data, term_col='Term', pvalue_col='Adjusted P-value', \n",
    "                         figsize=(10, 12), color='skyblue', \n",
    "                         title='Terms Sorted by Adjusted P-value',\n",
    "                         save_path=None, dpi=300, format='png',\n",
    "                         log_transform=False):\n",
    "    \"\"\"\n",
    "    Create a horizontal bar chart of Terms sorted by Adjusted P-value.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame or path to file\n",
    "        DataFrame containing the Term and Adjusted P-value columns\n",
    "    term_col : str, default='Term'\n",
    "        Name of the column containing the terms\n",
    "    pvalue_col : str, default='Adjusted P-value'\n",
    "        Name of the column containing the adjusted p-values\n",
    "    figsize : tuple, default=(10, 12)\n",
    "        Size of the figure (width, height)\n",
    "    color : str, default='skyblue'\n",
    "        Color of the bars\n",
    "    title : str, default='Terms Sorted by Adjusted P-value'\n",
    "        Title of the plot\n",
    "    save_path : str, default=None\n",
    "        Path to save the figure. If None, the figure is not saved\n",
    "    dpi : int, default=300\n",
    "        Resolution of the saved figure\n",
    "    format : str, default='png'\n",
    "        Format of the saved figure ('png', 'jpg', 'pdf', 'svg', etc.)\n",
    "    log_transform : bool, default=False\n",
    "        Whether to apply -log10 transformation to p-values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig, ax : matplotlib figure and axis objects\n",
    "    \"\"\"\n",
    "    # If data is a string, assume it's a file path and load it\n",
    "    if isinstance(data, str):\n",
    "        if data.endswith('.csv'):\n",
    "            df = pd.read_csv(data)\n",
    "        elif data.endswith(('.xls', '.xlsx')):\n",
    "            df = pd.read_excel(data)\n",
    "        else:\n",
    "            raise ValueError(\"File format not supported. Please provide a CSV or Excel file.\")\n",
    "    else:\n",
    "        # Create a deep copy to avoid modifying the original data\n",
    "        df = data.copy(deep=True)\n",
    "    \n",
    "    # Apply -log10 transformation if requested\n",
    "    if log_transform:\n",
    "        df[pvalue_col] = [-np.log10(x) for x in df[pvalue_col]]\n",
    "        x_label = '-log10(Adjusted P-value)'\n",
    "    else:\n",
    "        x_label = 'Adjusted P-value'\n",
    "    \n",
    "    # Sort by adjusted p-value in ascending order\n",
    "    df_sorted = df.sort_values(by=pvalue_col)\n",
    "    \n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    sns.barplot(x=pvalue_col, y=term_col, data=df_sorted, color=color, ax=ax)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel('Term')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add p-value annotations to the end of each bar\n",
    "    for i, p in enumerate(df_sorted[pvalue_col]):\n",
    "        ax.text(p + p*0.01, i, f'{p:.2e}', va='center')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure if a save path is provided\n",
    "    if save_path is not None:\n",
    "        # Create directory if it doesn't exist\n",
    "        save_dir = os.path.dirname(save_path)\n",
    "        if save_dir and not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        # Add file extension if not in the save_path\n",
    "        if not save_path.lower().endswith(f'.{format.lower()}'):\n",
    "            save_path = f\"{save_path}.{format}\"\n",
    "            \n",
    "        # Save the figure\n",
    "        fig.savefig(save_path, dpi=dpi, format=format, bbox_inches='tight')\n",
    "        print(f\"Figure saved to: {save_path}\")\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def draw_pvalue_analysis(summary):\n",
    "    save_info = \"\"\n",
    "    base_dir = \"files\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    for index, data in summary.items():\n",
    "        # 使用索引作为文件名的一部分，确保唯一性\n",
    "        save_path = os.path.join(base_dir, f\"pvalue_analysis_{index}_{current_time}\")\n",
    "        \n",
    "        # 使用log_transform参数而不是直接修改数据\n",
    "        fig, ax = plot_terms_by_pvalue(\n",
    "            data, \n",
    "            save_path=save_path,\n",
    "            log_transform=True,  # 在函数内部应用转换\n",
    "            title=f\"{index} - Terms Sorted by -log10(Adjusted P-value)\"\n",
    "        )\n",
    "        plt.close(fig)  # 关闭图形以释放内存\n",
    "        \n",
    "        save_info += f\"{index} 的富集结果图已保存到 {save_path}.png\\n\"\n",
    "    return save_info\n",
    "\n",
    "save_info = draw_pvalue_analysis(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "herb_names = [\"柴胡\", \"天花粉\", \"当归\", \"红花\", \"甘草\", \"大黄\", \"桃仁\"]\n",
    "\n",
    "# 读取herb_targets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
